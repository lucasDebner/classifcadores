# -*- coding: utf-8 -*-
"""
Comparativo completo — Naive Bayes, KNN e Decision Trees (insurance.csv)
Gera métricas por fold (accuracy, f1-weighted, f1-macro) e agregados.
"""

# Imports
import pandas as pd
import numpy as np
from google.colab import files
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt

# --------------------------
# Upload do arquivo (Colab)
# --------------------------
print("Envie insurance.csv")
uploaded = files.upload()
csv_name = [f for f in uploaded.keys() if f.endswith('.csv')][0]
df = pd.read_csv(csv_name)
print("Arquivo carregado:", csv_name)

# --------------------------
# Pré-processamento
# --------------------------
# Codificar categóricas (label encoding é suficiente para árvore; para KNN/NaiveBayes funciona também)
le = LabelEncoder()
df['sex'] = le.fit_transform(df['sex'])
df['smoker'] = le.fit_transform(df['smoker'])
df['region'] = le.fit_transform(df['region'])

# Discretizar charges em 3 faixas (você pode ajustar bins se quiser)
quant = np.quantile(df['charges'].values, [0.33, 0.66])
df['charges_class'] = np.digitize(df['charges'].values, quant)

# Features e target
feature_cols = ['age', 'bmi', 'children', 'smoker', 'region', 'sex']
X = df[feature_cols].values
y = df['charges_class'].astype(int).values

print("Distribuição das classes (0/1/2):", np.unique(y, return_counts=True))

# Scaling para KNN (mantemos X_scaler separado; árvores NÃO requerem scaling mas KNN sim)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --------------------------
# Preparar cross-validation
# --------------------------
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Armazenar métricas por fold em dicionário de dataframes
columns = ['fold', 'model', 'accuracy', 'f1_weighted', 'f1_macro']
records = []

fold_id = 0
for train_idx, test_idx in skf.split(X, y):
    fold_id += 1
    X_train_raw, X_test_raw = X[train_idx], X[test_idx]         # raw (no scaler)
    X_train_scaled, X_test_scaled = X_scaled[train_idx], X_scaled[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # 1) Naive Bayes (usamos raw ou scaled — GaussianNB tolera ambos; usar scaled para consistência)
    nb = GaussianNB()
    nb.fit(X_train_scaled, y_train)
    y_pred_nb = nb.predict(X_test_scaled)
    records.append([fold_id, 'Naive Bayes',
                    accuracy_score(y_test, y_pred_nb),
                    f1_score(y_test, y_pred_nb, average='weighted', zero_division=0),
                    f1_score(y_test, y_pred_nb, average='macro', zero_division=0)])

    # 2) KNN (usar scaled)
    knn = KNeighborsClassifier(n_neighbors=15)
    knn.fit(X_train_scaled, y_train)
    y_pred_knn = knn.predict(X_test_scaled)
    records.append([fold_id, 'KNN',
                    accuracy_score(y_test, y_pred_knn),
                    f1_score(y_test, y_pred_knn, average='weighted', zero_division=0),
                    f1_score(y_test, y_pred_knn, average='macro', zero_division=0)])

    # 3) Decision Tree (sem limite)
    dt0 = DecisionTreeClassifier(criterion='entropy', random_state=42)
    dt0.fit(X_train_raw, y_train)
    y_pred_dt0 = dt0.predict(X_test_raw)
    records.append([fold_id, 'DT (no limit)',
                    accuracy_score(y_test, y_pred_dt0),
                    f1_score(y_test, y_pred_dt0, average='weighted', zero_division=0),
                    f1_score(y_test, y_pred_dt0, average='macro', zero_division=0)])

    # 4) Decision Tree (max_depth=2)
    dt2 = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42)
    dt2.fit(X_train_raw, y_train)
    y_pred_dt2 = dt2.predict(X_test_raw)
    records.append([fold_id, 'DT (max_depth=2)',
                    accuracy_score(y_test, y_pred_dt2),
                    f1_score(y_test, y_pred_dt2, average='weighted', zero_division=0),
                    f1_score(y_test, y_pred_dt2, average='macro', zero_division=0)])

    # 5) Decision Tree (max_depth=3)
    dt3 = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
    dt3.fit(X_train_raw, y_train)
    y_pred_dt3 = dt3.predict(X_test_raw)
    records.append([fold_id, 'DT (max_depth=3)',
                    accuracy_score(y_test, y_pred_dt3),
                    f1_score(y_test, y_pred_dt3, average='weighted', zero_division=0),
                    f1_score(y_test, y_pred_dt3, average='macro', zero_division=0)])

# --------------------------
# Montar DataFrame de resultados
# --------------------------
df_results = pd.DataFrame(records, columns=columns)

# Estatísticas agregadas (média e desvio-padrão)
agg = df_results.groupby('model').agg(
    accuracy_mean=('accuracy','mean'),
    accuracy_std=('accuracy','std'),
    f1w_mean=('f1_weighted','mean'),
    f1w_std=('f1_weighted','std'),
    f1m_mean=('f1_macro','mean'),
    f1m_std=('f1_macro','std')
).reset_index()

print("\n===== Métricas agregadas por modelo (média ± std) =====")
for _, r in agg.iterrows():
    print(f"{r['model']}: accuracy={r['accuracy_mean']:.3f} ± {r['accuracy_std']:.3f}, "
          f"f1-weighted={r['f1w_mean']:.3f} ± {r['f1w_std']:.3f}, "
          f"f1-macro={r['f1m_mean']:.3f} ± {r['f1m_std']:.3f}")

# Mostrar tabela completa (por fold)
print("\n===== Resultados por fold (amostra) =====")
display(df_results.pivot(index='fold', columns='model', values='f1_macro'))

# --------------------------
# Plots comparativos
# --------------------------
plt.figure(figsize=(10,5))
df_results.boxplot(column='f1_macro', by='model', rot=45)
plt.title("Distribuição de F1-macro por modelo (10 folds)")
plt.suptitle('')
plt.ylabel('F1-macro')
plt.tight_layout()
plt.show()

# --------------------------
# Plot exemplar da árvore (max_depth=3) com classes nomeadas
# --------------------------
clf_final = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
clf_final.fit(X, y)

plt.figure(figsize=(20,10))
plot_tree(
    clf_final,
    filled=True,
    feature_names=feature_cols,
    class_names=['baixo', 'medio', 'alto'],
    rounded=True,
    fontsize=10
)
plt.title("Árvore de Decisão (max_depth=3) — classes nos nós folha")
plt.show()
